{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW01: Transfer Learning and Ensemble Methods for Medical Image Classification\n",
    "\n",
    "**Course**: CSYE 7374 - Deep Learning and Generative AI in Healthcare\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this homework, you will apply the transfer learning and ensemble techniques covered in class to a **different medical imaging dataset**. You will:\n",
    "\n",
    "1. Train **two** pretrained models on **BloodMNIST** (blood cell microscopy images)\n",
    "2. Implement and compare **ensemble methods**\n",
    "3. Analyze when and why ensemble learning outperforms individual models\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset: BloodMNIST\n",
    "\n",
    "**BloodMNIST** contains 17,092 microscopic images of blood cells categorized into **8 classes**:\n",
    "- 0: basophil\n",
    "- 1: eosinophil\n",
    "- 2: erythroblast\n",
    "- 3: immature granulocyte (ig)\n",
    "- 4: lymphocyte\n",
    "- 5: monocyte\n",
    "- 6: neutrophil\n",
    "- 7: platelet\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Complete all cells marked with **`# TODO`**\n",
    "- Do not modify the provided helper functions unless instructed\n",
    "- Run all cells in order\n",
    "- Answer the analysis questions at the end\n",
    "\n",
    "---\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "| Task | Points |\n",
    "|------|--------|\n",
    "| Data loading and visualization | 10 |\n",
    "| Data preprocessing (val_transform) | 5 |\n",
    "| Model definition (TransferLearningModel class) | 15 |\n",
    "| Loss function, optimizer, and scheduler | 10 |\n",
    "| Training and validation functions | 15 |\n",
    "| Model training and learning curves | 10 |\n",
    "| Ensemble implementation (average + voting) | 10 |\n",
    "| Ensemble evaluation on test set | 5 |\n",
    "| Analysis questions (4 x 5 points) | 20 |\n",
    "| **Total** | **100** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q torch torchvision medmnist matplotlib seaborn scikit-learn tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Torchvision - transforms and pretrained models\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "# TODO: Import additional models as needed for your second model choice\n",
    "# from torchvision.models import vgg19, VGG19_Weights\n",
    "# from torchvision.models import googlenet, GoogLeNet_Weights\n",
    "# from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "# MedMNIST dataset\n",
    "import medmnist\n",
    "from medmnist import INFO\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    accuracy_score, precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"MedMNIST: {medmnist.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_FLAG = 'bloodmnist'  # BloodMNIST dataset\n",
    "    DOWNLOAD = True\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 15  # Reduced for homework\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    IMG_SIZE = 224\n",
    "    SEED = 42\n",
    "    CHECKPOINT_DIR = './checkpoints_hw'\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(Config.SEED)\n",
    "os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset information\n",
    "info = INFO[Config.DATA_FLAG]\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "class_names = list(info['label'].values())\n",
    "\n",
    "print(f\"Dataset: {Config.DATA_FLAG.upper()}\")\n",
    "print(f\"Classes: {n_classes}\")\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw datasets\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "train_dataset_raw = DataClass(split='train', download=Config.DOWNLOAD)\n",
    "\n",
    "# ============================================================\n",
    "# TODO: Load the validation and test datasets\n",
    "# ============================================================\n",
    "val_dataset_raw = None  # TODO\n",
    "test_dataset_raw = None  # TODO\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nTrain: {len(train_dataset_raw)} | Val: {len(val_dataset_raw)} | Test: {len(test_dataset_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Images from Each Class\n",
    "\n",
    "Complete the function below to display **3 sample images from each of the 8 classes** in a grid layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Visualize sample images from each class\n",
    "# ============================================================\n",
    "\n",
    "def visualize_samples_per_class(dataset, class_names, n_samples=3):\n",
    "    \"\"\"Visualize n_samples from each class.\"\"\"\n",
    "    n_classes = len(class_names)\n",
    "    fig, axes = plt.subplots(n_classes, n_samples, figsize=(n_samples * 2, n_classes * 2))\n",
    "    \n",
    "    # TODO: Collect samples for each class and plot them\n",
    "    \n",
    "    pass  # Remove this line after completing TODO\n",
    "    \n",
    "    plt.suptitle('Sample Images from Each Class', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call your function\n",
    "# visualize_samples_per_class(train_dataset_raw, class_names, n_samples=3)\n",
    "\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "def get_class_distribution(dataset):\n",
    "    labels = [label[0] for _, label in dataset]\n",
    "    return np.bincount(labels, minlength=n_classes)\n",
    "\n",
    "train_dist = get_class_distribution(train_dataset_raw)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(n_classes), train_dist, color=sns.color_palette(\"husl\", n_classes))\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Training Set Class Distribution')\n",
    "plt.xticks(range(n_classes), [f'{i}' for i in range(n_classes)])\n",
    "plt.show()\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}: {train_dist[i]} ({train_dist[i]/train_dist.sum()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet normalization\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Training transforms (with data augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# ============================================================\n",
    "# TODO: Complete the validation/test transform (no augmentation)\n",
    "# ============================================================\n",
    "val_transform = None  # TODO\n",
    "# ============================================================\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DataClass(split='train', transform=train_transform, download=Config.DOWNLOAD)\n",
    "val_dataset = DataClass(split='val', transform=val_transform, download=Config.DOWNLOAD)\n",
    "test_dataset = DataClass(split='test', transform=val_transform, download=Config.DOWNLOAD)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Data loaders created: {len(train_loader)} train batches, {len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Definitions\n",
    "\n",
    "### Complete the TransferLearningModel class\n",
    "\n",
    "You need to implement **two models**:\n",
    "1. **ResNet50** (required)\n",
    "2. **One model of your choice**\n",
    "\n",
    "Complete the model initialization code below by modifying the final classification layer to output `num_classes` instead of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearningModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, pretrained=True):\n",
    "        super(TransferLearningModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # ============================================================\n",
    "        # TODO: Load pretrained model and modify the final layer\n",
    "        # ============================================================\n",
    "        \n",
    "        if model_name == 'resnet50':\n",
    "            weights = ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
    "            self.model = resnet50(weights=weights)\n",
    "            # TODO: Modify the final layer\n",
    "            \n",
    "        # TODO: Add elif blocks for your second model choice\n",
    "        # elif model_name == '...':\n",
    "        #     ...\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "        # ============================================================\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Create your two model instances\n",
    "# Model 1 must be ResNet50\n",
    "# Model 2 is your choice\n",
    "# ============================================================\n",
    "\n",
    "# Model 1: ResNet50 (required)\n",
    "model_1_name = 'ResNet50'\n",
    "model_1 = None  # TODO\n",
    "\n",
    "# Model 2: Your choice\n",
    "model_2_name = 'YourChoice'  # TODO: Change name\n",
    "model_2 = None  # TODO\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "# Store models in a dictionary\n",
    "models_dict = {\n",
    "    model_1_name: model_1,\n",
    "    model_2_name: model_2\n",
    "}\n",
    "\n",
    "# Print model parameters\n",
    "for name, model in models_dict.items():\n",
    "    if model is not None:\n",
    "        total = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"{name}: {total:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Loss Function and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights for imbalanced data\n",
    "def calculate_class_weights(distribution):\n",
    "    total = distribution.sum()\n",
    "    weights = total / (len(distribution) * distribution)\n",
    "    weights = weights / weights.sum() * len(weights)\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "class_weights = calculate_class_weights(train_dist)\n",
    "\n",
    "# ============================================================\n",
    "# TODO: Define the loss function with class weights\n",
    "# ============================================================\n",
    "criterion = None  # TODO\n",
    "# ============================================================\n",
    "\n",
    "# Create optimizers and schedulers for each model\n",
    "optimizers = {}\n",
    "schedulers = {}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    if model is not None:\n",
    "        # ============================================================\n",
    "        # TODO: Create optimizer and learning rate scheduler\n",
    "        # ============================================================\n",
    "        optimizers[name] = None  # TODO\n",
    "        schedulers[name] = None  # TODO\n",
    "        # ============================================================\n",
    "\n",
    "print(\"Loss and optimizers configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, model_name):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc='Training', leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.squeeze().long().to(device)\n",
    "        \n",
    "        # ============================================================\n",
    "        # TODO: Complete the training step\n",
    "        # ============================================================\n",
    "        \n",
    "        outputs = None  # TODO\n",
    "        loss = None  # TODO\n",
    "        \n",
    "        # ============================================================\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / total, 100. * correct / total\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    # ============================================================\n",
    "    # TODO: Complete the validation loop\n",
    "    # ============================================================\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc='Validating', leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.squeeze().long().to(device)\n",
    "            \n",
    "            outputs = None  # TODO\n",
    "            loss = None  # TODO\n",
    "            probs = None  # TODO\n",
    "            predicted = None  # TODO\n",
    "            \n",
    "            # Track statistics (do not modify)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # ============================================================\n",
    "    \n",
    "    return running_loss / total, 100. * correct / total, np.array(all_preds), np.array(all_labels), np.array(all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0.0\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\nTraining {model_name}\\n{'='*50}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, model_name)\n",
    "        val_loss, val_acc, _, _, _ = validate(model, val_loader, criterion, device)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train: {train_acc:.1f}% | Val: {val_acc:.1f}%\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    model.load_state_dict(best_weights)\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Train Models\n",
    "\n",
    "Run training for both models. This may take 10-20 minutes per model depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Train both models and store histories\n",
    "# ============================================================\n",
    "\n",
    "all_histories = {}\n",
    "\n",
    "# TODO: Train each model in models_dict\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Plot learning curves (loss and accuracy)\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# TODO: Plot validation loss and accuracy for each model\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Ensemble Methods\n",
    "\n",
    "### Implement the Ensemble Class\n",
    "\n",
    "Complete the `ensemble_average` and `ensemble_voting` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    def __init__(self, models_dict, device):\n",
    "        self.models = models_dict\n",
    "        self.device = device\n",
    "    \n",
    "    def predict_proba(self, images, model_name):\n",
    "        \"\"\"Get probability predictions from a single model.\"\"\"\n",
    "        model = self.models[model_name]\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "        return probs\n",
    "    \n",
    "    def ensemble_average(self, images):\n",
    "        \"\"\"\n",
    "        Ensemble by averaging probabilities from all models.\n",
    "        \n",
    "        Returns:\n",
    "            predictions: argmax of averaged probabilities\n",
    "            avg_probs: averaged probability distribution\n",
    "        \"\"\"\n",
    "        # ============================================================\n",
    "        # TODO: Implement probability averaging ensemble\n",
    "        # ============================================================\n",
    "        \n",
    "        # TODO: Return predictions and averaged probabilities\n",
    "        \n",
    "        pass  # Remove this line after completing TODO\n",
    "        # ============================================================\n",
    "    \n",
    "    def ensemble_voting(self, images):\n",
    "        \"\"\"\n",
    "        Ensemble by majority voting.\n",
    "        \n",
    "        Returns:\n",
    "            votes: majority vote predictions\n",
    "            avg_probs: averaged probabilities (for confidence)\n",
    "        \"\"\"\n",
    "        # ============================================================\n",
    "        # TODO: Implement majority voting ensemble\n",
    "        # ============================================================\n",
    "        \n",
    "        # TODO: Return majority vote predictions and probabilities\n",
    "        \n",
    "        pass  # Remove this line after completing TODO\n",
    "        # ============================================================\n",
    "\n",
    "# Create ensemble\n",
    "ensemble = EnsembleModel(models_dict, device)\n",
    "print(\"Ensemble model created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Evaluate All Models on Test Set\n",
    "\n",
    "### Complete the evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate individual models\n",
    "individual_results = {}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    if model is not None:\n",
    "        _, test_acc, preds, labels, probs = validate(model, test_loader, criterion, device)\n",
    "        individual_results[name] = {\n",
    "            'accuracy': test_acc,\n",
    "            'predictions': preds,\n",
    "            'labels': labels,\n",
    "            'probabilities': probs\n",
    "        }\n",
    "        print(f\"{name} Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Evaluate ensemble methods on test set\n",
    "# ============================================================\n",
    "\n",
    "ensemble_results = {}\n",
    "\n",
    "# TODO: Evaluate both 'average' and 'voting' ensemble methods\n",
    "# Store results in ensemble_results dictionary\n",
    "\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best individual model\n",
    "best_individual = max(individual_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "best_name, best_results = best_individual\n",
    "\n",
    "cm = confusion_matrix(best_results['labels'], best_results['predictions'])\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f'Confusion Matrix - {best_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification reports\n",
    "print(\"=\"*60)\n",
    "print(f\"Classification Report: {best_name}\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(best_results['labels'], best_results['predictions'], \n",
    "                            target_names=class_names, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_results = {**individual_results, **ensemble_results}\n",
    "\n",
    "results_data = []\n",
    "for name, res in all_results.items():\n",
    "    model_type = 'Ensemble' if 'Ensemble' in name else 'Individual'\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        res['labels'], res['predictions'], average='macro'\n",
    "    )\n",
    "    results_data.append({\n",
    "        'Model': name,\n",
    "        'Type': model_type,\n",
    "        'Accuracy (%)': res['accuracy'],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data).sort_values('Accuracy (%)', ascending=False)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#2196F3' if 'Ensemble' not in m else '#4CAF50' for m in results_df['Model']]\n",
    "bars = plt.barh(results_df['Model'], results_df['Accuracy (%)'], color=colors)\n",
    "plt.xlabel('Test Accuracy (%)')\n",
    "plt.title('Model Performance Comparison')\n",
    "\n",
    "for bar, acc in zip(bars, results_df['Accuracy (%)']):\n",
    "    plt.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2, \n",
    "             f'{acc:.1f}%', va='center')\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "plt.legend(handles=[Patch(color='#2196F3', label='Individual'), \n",
    "                    Patch(color='#4CAF50', label='Ensemble')])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Analysis Questions (20 points)\n",
    "\n",
    "Answer the following questions based on your results. Write your answers in the markdown cells below each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (5 points)\n",
    "\n",
    "Compare the test accuracy of your two individual models. Which model performed better? Provide at least **two possible reasons** why one model outperformed the other, considering factors like model architecture, number of parameters, or training dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "*[Write your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (5 points)\n",
    "\n",
    "Did the ensemble methods outperform the individual models? Compare the **average probability** ensemble vs **majority voting** ensemble. Which performed better and why might this be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "*[Write your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (5 points)\n",
    "\n",
    "Looking at the confusion matrix and per-class metrics, which blood cell types were **most difficult to classify**? Propose one reason why these classes might be challenging and one potential solution to improve classification for these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "*[Write your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (5 points)\n",
    "\n",
    "In this homework, we used only **two models** for the ensemble. Based on the class material (which used four models), do you think adding more models would improve ensemble performance? What are the **trade-offs** of using more models in an ensemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**\n",
    "\n",
    "*[Write your answer here]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission Instructions\n",
    "\n",
    "1. Run all cells and ensure there are no errors\n",
    "2. Rename this notebook as: **`FirstName_LastName_HW01.ipynb`**\n",
    "3. Download and submit to the course portal by the deadline\n",
    "\n",
    "**Example filename:** `John_Smith_HW01.ipynb`\n",
    "\n",
    "**Make sure your notebook shows:**\n",
    "- All TODOs completed\n",
    "- Training output for both models\n",
    "- All visualizations rendered\n",
    "- Analysis questions answered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
